{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce6904c",
   "metadata": {},
   "source": [
    "# Hyperliquid Sentiment (Fear/Greed) Analysis\n",
    "\n",
    "This notebook analyzes how market sentiment relates to trader behavior and performance on Hyperliquid.\n",
    "\n",
    "## Scope\n",
    "- Data quality checks (rows/columns, missing values, duplicates)\n",
    "- Daily alignment of trade and sentiment data\n",
    "- Key trader metrics (PnL, win rate, trade size, leverage proxy, activity, long/short bias)\n",
    "- Fear vs Greed comparisons with bootstrap confidence intervals\n",
    "- Trader segmentation (leverage, activity, consistency)\n",
    "- Bonus predictive model for next-day profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "TRADES_PATH = ROOT / 'historical_data.csv'\n",
    "SENTIMENT_PATH = ROOT / 'fear_greed_index.csv'\n",
    "OUTPUT_DIR = ROOT / 'outputs'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "CHARTS_DIR = OUTPUT_DIR / 'charts'\n",
    "CLEANED_DIR = OUTPUT_DIR / 'cleaned'\n",
    "\n",
    "for path in (OUTPUT_DIR, TABLES_DIR, CHARTS_DIR, CLEANED_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Working directory: {ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataframe(name: str, df: pd.DataFrame) -> dict[str, float]:\n",
    "    total_missing = int(df.isna().sum().sum())\n",
    "    return {\n",
    "        'dataset': name,\n",
    "        'rows': int(df.shape[0]),\n",
    "        'columns': int(df.shape[1]),\n",
    "        'duplicate_rows': int(df.duplicated().sum()),\n",
    "        'total_missing_cells': total_missing,\n",
    "        'missing_cell_pct': float(total_missing / (df.shape[0] * df.shape[1])),\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_diff(\n",
    "    fear: pd.Series,\n",
    "    greed: pd.Series,\n",
    "    stat_fn: Callable[[np.ndarray], float],\n",
    "    n_boot: int = 5000,\n",
    "    seed: int = 42,\n",
    ") -> dict[str, float]:\n",
    "    fear_values = fear.dropna().to_numpy()\n",
    "    greed_values = greed.dropna().to_numpy()\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    diffs = np.empty(n_boot)\n",
    "    for idx in range(n_boot):\n",
    "        fear_sample = rng.choice(fear_values, size=len(fear_values), replace=True)\n",
    "        greed_sample = rng.choice(greed_values, size=len(greed_values), replace=True)\n",
    "        diffs[idx] = stat_fn(fear_sample) - stat_fn(greed_sample)\n",
    "\n",
    "    return {\n",
    "        'fear_minus_greed': float(np.mean(diffs)),\n",
    "        'ci_low_95': float(np.quantile(diffs, 0.025)),\n",
    "        'ci_high_95': float(np.quantile(diffs, 0.975)),\n",
    "    }\n",
    "\n",
    "\n",
    "def expected_shortfall_10(values: pd.Series) -> float:\n",
    "    values = values.dropna()\n",
    "    if values.empty:\n",
    "        return float('nan')\n",
    "    p10 = values.quantile(0.10)\n",
    "    return float(values[values <= p10].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b28ffd",
   "metadata": {},
   "source": [
    "## Part A: Data Preparation\n",
    "\n",
    "Load both datasets, check quality, normalize timestamps, align on daily date, and create key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_raw = pd.read_csv(TRADES_PATH)\n",
    "sentiment_raw = pd.read_csv(SENTIMENT_PATH)\n",
    "\n",
    "quality_df = pd.DataFrame([\n",
    "    profile_dataframe('historical_data', trades_raw),\n",
    "    profile_dataframe('fear_greed_index', sentiment_raw),\n",
    "])\n",
    "quality_df.to_csv(TABLES_DIR / 'data_quality_summary.csv', index=False)\n",
    "\n",
    "print('Data quality summary:')\n",
    "display(quality_df)\n",
    "\n",
    "print('\\nTrades columns:')\n",
    "print(trades_raw.columns.tolist())\n",
    "print('\\nSentiment columns:')\n",
    "print(sentiment_raw.columns.tolist())\n",
    "\n",
    "# Timestamp integrity check\n",
    "print('\\nTimestamp uniqueness check:')\n",
    "print('Timestamp unique count:', trades_raw['Timestamp'].nunique())\n",
    "print('Timestamp IST unique count:', trades_raw['Timestamp IST'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(trades_raw: pd.DataFrame, sentiment_raw: pd.DataFrame):\n",
    "    trades = trades_raw.copy()\n",
    "    sentiment = sentiment_raw.copy()\n",
    "\n",
    "    trades['timestamp_ist'] = pd.to_datetime(\n",
    "        trades['Timestamp IST'], format='%d-%m-%Y %H:%M', errors='coerce'\n",
    "    )\n",
    "    trades['timestamp_ms'] = pd.to_datetime(\n",
    "        trades['Timestamp'], unit='ms', errors='coerce', utc=True\n",
    "    )\n",
    "    trades['date'] = trades['timestamp_ist'].dt.date\n",
    "\n",
    "    sentiment['date'] = pd.to_datetime(sentiment['date'], errors='coerce').dt.date\n",
    "    sentiment['sentiment_bucket'] = sentiment['classification'].replace({\n",
    "        'Extreme Fear': 'Fear',\n",
    "        'Extreme Greed': 'Greed',\n",
    "    })\n",
    "\n",
    "    merged = trades.merge(\n",
    "        sentiment[['date', 'value', 'classification', 'sentiment_bucket']],\n",
    "        on='date',\n",
    "        how='left',\n",
    "    )\n",
    "\n",
    "    side = merged['Side'].str.upper()\n",
    "    merged['side_sign'] = np.where(side.eq('BUY'), 1, -1)\n",
    "    merged['signed_size_usd'] = merged['Size USD'] * merged['side_sign']\n",
    "    merged['signed_size_tokens'] = merged['Size Tokens'] * merged['side_sign']\n",
    "\n",
    "    merged['pos_before_usd'] = merged['Start Position'].abs() * merged['Execution Price']\n",
    "    merged['pos_after_tokens'] = merged['Start Position'] + merged['signed_size_tokens']\n",
    "    merged['pos_after_usd'] = merged['pos_after_tokens'].abs() * merged['Execution Price']\n",
    "\n",
    "    # Leverage proxy because explicit account-level leverage/equity is not provided.\n",
    "    merged['leverage_proxy'] = (\n",
    "        merged['pos_after_usd'] / (merged['pos_before_usd'] + 1.0)\n",
    "    ).clip(lower=0, upper=50)\n",
    "\n",
    "    merged['is_realized'] = (merged['Closed PnL'] != 0).astype(int)\n",
    "    merged['is_win'] = (merged['Closed PnL'] > 0).astype(int)\n",
    "    merged['is_loss'] = (merged['Closed PnL'] < 0).astype(int)\n",
    "\n",
    "    daily = (\n",
    "        merged.groupby(['Account', 'date', 'sentiment_bucket'], dropna=False, as_index=False)\n",
    "        .agg(\n",
    "            trades=('Trade ID', 'count'),\n",
    "            daily_pnl_usd=('Closed PnL', 'sum'),\n",
    "            avg_trade_size_usd=('Size USD', 'mean'),\n",
    "            total_notional_usd=('Size USD', 'sum'),\n",
    "            realized_trades=('is_realized', 'sum'),\n",
    "            wins=('is_win', 'sum'),\n",
    "            losses=('is_loss', 'sum'),\n",
    "            buy_notional_usd=('signed_size_usd', lambda x: x[x > 0].sum()),\n",
    "            sell_notional_usd=('signed_size_usd', lambda x: -x[x < 0].sum()),\n",
    "            avg_leverage_proxy=('leverage_proxy', 'mean'),\n",
    "            p90_leverage_proxy=('leverage_proxy', lambda x: x.quantile(0.90)),\n",
    "            total_fees_usd=('Fee', 'sum'),\n",
    "        )\n",
    "        .sort_values(['Account', 'date'])\n",
    "    )\n",
    "\n",
    "    daily['win_rate'] = np.where(\n",
    "        daily['realized_trades'] > 0,\n",
    "        daily['wins'] / daily['realized_trades'],\n",
    "        np.nan,\n",
    "    )\n",
    "    daily['long_short_ratio'] = daily['buy_notional_usd'] / daily['sell_notional_usd'].replace(0, np.nan)\n",
    "    daily['net_long_bias'] = (\n",
    "        (daily['buy_notional_usd'] - daily['sell_notional_usd'])\n",
    "        / (daily['buy_notional_usd'] + daily['sell_notional_usd'] + 1e-9)\n",
    "    )\n",
    "\n",
    "    daily['cum_pnl_usd'] = daily.groupby('Account')['daily_pnl_usd'].cumsum()\n",
    "    daily['running_peak_usd'] = daily.groupby('Account')['cum_pnl_usd'].cummax()\n",
    "    daily['drawdown_usd'] = daily['cum_pnl_usd'] - daily['running_peak_usd']\n",
    "\n",
    "    return merged, daily, sentiment\n",
    "\n",
    "\n",
    "merged, daily, sentiment_clean = prepare_data(trades_raw, sentiment_raw)\n",
    "\n",
    "merged.to_csv(CLEANED_DIR / 'trades_enriched.csv', index=False)\n",
    "daily.to_csv(CLEANED_DIR / 'daily_account_metrics.csv', index=False)\n",
    "\n",
    "print('Prepared datasets:')\n",
    "print('Merged rows:', len(merged))\n",
    "print('Daily account rows:', len(daily))\n",
    "print('Unmatched sentiment rows:', int(merged['sentiment_bucket'].isna().sum()))\n",
    "\n",
    "display(daily.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b146fc",
   "metadata": {},
   "source": [
    "## Part B: Fear vs Greed Analysis\n",
    "\n",
    "Answer whether performance and behavior differ between Fear and Greed regimes, and quantify those differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfe930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fear_greed_tables(daily: pd.DataFrame):\n",
    "    fg_daily = daily[daily['sentiment_bucket'].isin(['Fear', 'Greed'])].copy()\n",
    "\n",
    "    performance = (\n",
    "        fg_daily.groupby('sentiment_bucket', as_index=False)\n",
    "        .agg(\n",
    "            account_days=('Account', 'count'),\n",
    "            traders=('Account', 'nunique'),\n",
    "            mean_daily_pnl_usd=('daily_pnl_usd', 'mean'),\n",
    "            median_daily_pnl_usd=('daily_pnl_usd', 'median'),\n",
    "            positive_day_rate=('daily_pnl_usd', lambda s: (s > 0).mean()),\n",
    "            mean_win_rate=('win_rate', 'mean'),\n",
    "            mean_drawdown_usd=('drawdown_usd', 'mean'),\n",
    "            p10_drawdown_usd=('drawdown_usd', lambda s: s.quantile(0.10)),\n",
    "            p05_daily_pnl_usd=('daily_pnl_usd', lambda s: s.quantile(0.05)),\n",
    "            es10_daily_pnl_usd=('daily_pnl_usd', expected_shortfall_10),\n",
    "        )\n",
    "        .sort_values('sentiment_bucket')\n",
    "    )\n",
    "\n",
    "    behavior = (\n",
    "        fg_daily.groupby('sentiment_bucket', as_index=False)\n",
    "        .agg(\n",
    "            mean_trades_per_account_day=('trades', 'mean'),\n",
    "            median_trades_per_account_day=('trades', 'median'),\n",
    "            mean_trade_size_usd=('avg_trade_size_usd', 'mean'),\n",
    "            median_trade_size_usd=('avg_trade_size_usd', 'median'),\n",
    "            mean_total_notional_usd=('total_notional_usd', 'mean'),\n",
    "            mean_leverage_proxy=('avg_leverage_proxy', 'mean'),\n",
    "            median_leverage_proxy=('avg_leverage_proxy', 'median'),\n",
    "            median_long_short_ratio=(\n",
    "                'long_short_ratio',\n",
    "                lambda s: s.replace([np.inf, -np.inf], np.nan).median(),\n",
    "            ),\n",
    "            mean_long_short_ratio_capped=(\n",
    "                'long_short_ratio',\n",
    "                lambda s: s.replace([np.inf, -np.inf], np.nan).clip(0, 5).mean(),\n",
    "            ),\n",
    "            mean_net_long_bias=('net_long_bias', 'mean'),\n",
    "            share_net_long_days=('net_long_bias', lambda s: (s > 0).mean()),\n",
    "        )\n",
    "        .sort_values('sentiment_bucket')\n",
    "    )\n",
    "\n",
    "    fear = fg_daily[fg_daily['sentiment_bucket'] == 'Fear']\n",
    "    greed = fg_daily[fg_daily['sentiment_bucket'] == 'Greed']\n",
    "\n",
    "    tests = []\n",
    "    metric_specs = [\n",
    "        ('daily_pnl_usd', 'median_daily_pnl_usd', np.median),\n",
    "        ('win_rate', 'mean_win_rate', np.mean),\n",
    "        ('drawdown_usd', 'p10_drawdown_usd', lambda x: float(np.quantile(x, 0.10))),\n",
    "        ('trades', 'mean_trades_per_account_day', np.mean),\n",
    "        ('avg_trade_size_usd', 'mean_trade_size_usd', np.mean),\n",
    "        ('avg_leverage_proxy', 'mean_leverage_proxy', np.mean),\n",
    "        ('net_long_bias', 'mean_net_long_bias', np.mean),\n",
    "    ]\n",
    "\n",
    "    for column, label, fn in metric_specs:\n",
    "        stats = bootstrap_diff(fear[column], greed[column], fn)\n",
    "        tests.append({\n",
    "            'metric': label,\n",
    "            'fear_minus_greed': stats['fear_minus_greed'],\n",
    "            'ci_low_95': stats['ci_low_95'],\n",
    "            'ci_high_95': stats['ci_high_95'],\n",
    "        })\n",
    "\n",
    "    return fg_daily, performance, behavior, pd.DataFrame(tests)\n",
    "\n",
    "\n",
    "fg_daily, performance_df, behavior_df, tests_df = fear_greed_tables(daily)\n",
    "\n",
    "performance_df.to_csv(TABLES_DIR / 'performance_fear_vs_greed.csv', index=False)\n",
    "behavior_df.to_csv(TABLES_DIR / 'behavior_fear_vs_greed.csv', index=False)\n",
    "tests_df.to_csv(TABLES_DIR / 'bootstrap_differences.csv', index=False)\n",
    "\n",
    "print('Performance summary:')\n",
    "display(performance_df)\n",
    "print('\\nBehavior summary:')\n",
    "display(behavior_df)\n",
    "print('\\nBootstrap differences (Fear - Greed):')\n",
    "display(tests_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683338ff",
   "metadata": {},
   "source": [
    "## Segment Analysis\n",
    "\n",
    "Build segments for leverage, activity, and consistency; compare each segment across sentiment regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65676766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_tables(daily: pd.DataFrame):\n",
    "    fg_daily = daily[daily['sentiment_bucket'].isin(['Fear', 'Greed'])].copy()\n",
    "\n",
    "    trader_profile = (\n",
    "        fg_daily.groupby('Account', as_index=False)\n",
    "        .agg(\n",
    "            active_days=('date', 'nunique'),\n",
    "            total_trades=('trades', 'sum'),\n",
    "            avg_trades_per_day=('trades', 'mean'),\n",
    "            avg_trade_size_usd=('avg_trade_size_usd', 'mean'),\n",
    "            avg_leverage_proxy=('avg_leverage_proxy', 'mean'),\n",
    "            positive_day_rate=('daily_pnl_usd', lambda s: (s > 0).mean()),\n",
    "            total_pnl_usd=('daily_pnl_usd', 'sum'),\n",
    "            mean_daily_pnl_usd=('daily_pnl_usd', 'mean'),\n",
    "            pnl_volatility_usd=('daily_pnl_usd', 'std'),\n",
    "            mean_win_rate=('win_rate', 'mean'),\n",
    "        )\n",
    "        .sort_values('Account')\n",
    "    )\n",
    "\n",
    "    trader_profile['pnl_volatility_usd'] = trader_profile['pnl_volatility_usd'].fillna(0)\n",
    "    trader_profile['mean_win_rate'] = trader_profile['mean_win_rate'].fillna(0)\n",
    "\n",
    "    lev_q70 = float(trader_profile['avg_leverage_proxy'].quantile(0.70))\n",
    "    freq_q70 = float(trader_profile['avg_trades_per_day'].quantile(0.70))\n",
    "    cons_q70 = float(trader_profile['positive_day_rate'].quantile(0.70))\n",
    "\n",
    "    trader_profile['leverage_segment'] = np.where(\n",
    "        trader_profile['avg_leverage_proxy'] >= lev_q70,\n",
    "        'High leverage-proxy',\n",
    "        'Low leverage-proxy',\n",
    "    )\n",
    "    trader_profile['activity_segment'] = np.where(\n",
    "        trader_profile['avg_trades_per_day'] >= freq_q70,\n",
    "        'Frequent',\n",
    "        'Infrequent',\n",
    "    )\n",
    "    trader_profile['consistency_segment'] = np.where(\n",
    "        trader_profile['positive_day_rate'] >= cons_q70,\n",
    "        'Consistent winners',\n",
    "        'Inconsistent',\n",
    "    )\n",
    "\n",
    "    segmented = fg_daily.merge(\n",
    "        trader_profile[['Account', 'leverage_segment', 'activity_segment', 'consistency_segment']],\n",
    "        on='Account',\n",
    "        how='left',\n",
    "    )\n",
    "\n",
    "    frames = []\n",
    "    for segment_col in ('leverage_segment', 'activity_segment', 'consistency_segment'):\n",
    "        grouped = (\n",
    "            segmented.groupby(['sentiment_bucket', segment_col], as_index=False)\n",
    "            .agg(\n",
    "                account_days=('Account', 'count'),\n",
    "                traders=('Account', 'nunique'),\n",
    "                mean_daily_pnl_usd=('daily_pnl_usd', 'mean'),\n",
    "                median_daily_pnl_usd=('daily_pnl_usd', 'median'),\n",
    "                positive_day_rate=('daily_pnl_usd', lambda s: (s > 0).mean()),\n",
    "                mean_win_rate=('win_rate', 'mean'),\n",
    "                mean_trades=('trades', 'mean'),\n",
    "                mean_trade_size_usd=('avg_trade_size_usd', 'mean'),\n",
    "                mean_leverage_proxy=('avg_leverage_proxy', 'mean'),\n",
    "                mean_net_long_bias=('net_long_bias', 'mean'),\n",
    "                es10_daily_pnl_usd=('daily_pnl_usd', expected_shortfall_10),\n",
    "            )\n",
    "            .rename(columns={segment_col: 'segment'})\n",
    "        )\n",
    "        grouped['segment_type'] = segment_col\n",
    "        frames.append(grouped)\n",
    "\n",
    "    segment_summary = pd.concat(frames, ignore_index=True)\n",
    "    thresholds = pd.DataFrame([\n",
    "        {'threshold_name': 'leverage_q70', 'value': lev_q70},\n",
    "        {'threshold_name': 'activity_q70', 'value': freq_q70},\n",
    "        {'threshold_name': 'consistency_q70', 'value': cons_q70},\n",
    "    ])\n",
    "\n",
    "    return trader_profile, segment_summary, thresholds\n",
    "\n",
    "\n",
    "trader_profile_df, segment_summary_df, thresholds_df = segment_tables(daily)\n",
    "\n",
    "trader_profile_df.to_csv(TABLES_DIR / 'trader_profiles.csv', index=False)\n",
    "segment_summary_df.to_csv(TABLES_DIR / 'segment_performance.csv', index=False)\n",
    "thresholds_df.to_csv(TABLES_DIR / 'segment_thresholds.csv', index=False)\n",
    "\n",
    "print('Segment thresholds:')\n",
    "display(thresholds_df)\n",
    "\n",
    "print('\\nSegment performance (sample):')\n",
    "display(segment_summary_df.head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11abecb0",
   "metadata": {},
   "source": [
    "## Bonus: Predictive Model\n",
    "\n",
    "Predict next-day profitability using sentiment + lagged behavior features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e7f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predictive_model(daily: pd.DataFrame, sentiment: pd.DataFrame):\n",
    "    model_df = daily.merge(sentiment[['date', 'value']], on='date', how='left')\n",
    "    model_df = model_df[model_df['sentiment_bucket'].isin(['Fear', 'Greed', 'Neutral'])].copy()\n",
    "    model_df = model_df.sort_values(['Account', 'date'])\n",
    "\n",
    "    model_df['next_day_pnl_usd'] = model_df.groupby('Account')['daily_pnl_usd'].shift(-1)\n",
    "    model_df['next_day_profitable'] = (model_df['next_day_pnl_usd'] > 0).astype(int)\n",
    "\n",
    "    lag_cols = [\n",
    "        'daily_pnl_usd',\n",
    "        'trades',\n",
    "        'avg_trade_size_usd',\n",
    "        'total_notional_usd',\n",
    "        'avg_leverage_proxy',\n",
    "        'net_long_bias',\n",
    "    ]\n",
    "    for col in lag_cols:\n",
    "        model_df[f'lag1_{col}'] = model_df.groupby('Account')[col].shift(1)\n",
    "\n",
    "    model_df = model_df.dropna(subset=['next_day_pnl_usd']).copy()\n",
    "\n",
    "    features = [\n",
    "        'value',\n",
    "        'sentiment_bucket',\n",
    "        'lag1_daily_pnl_usd',\n",
    "        'lag1_trades',\n",
    "        'lag1_avg_trade_size_usd',\n",
    "        'lag1_total_notional_usd',\n",
    "        'lag1_avg_leverage_proxy',\n",
    "        'lag1_net_long_bias',\n",
    "    ]\n",
    "\n",
    "    use_df = model_df.dropna(subset=['date']).copy()\n",
    "    unique_dates = sorted(use_df['date'].unique())\n",
    "    split_idx = int(len(unique_dates) * 0.80)\n",
    "    split_date = unique_dates[split_idx]\n",
    "\n",
    "    train_mask = use_df['date'] < split_date\n",
    "    test_mask = use_df['date'] >= split_date\n",
    "\n",
    "    X_train = use_df.loc[train_mask, features]\n",
    "    y_train = use_df.loc[train_mask, 'next_day_profitable']\n",
    "    X_test = use_df.loc[test_mask, features]\n",
    "    y_test = use_df.loc[test_mask, 'next_day_profitable']\n",
    "\n",
    "    num_cols = [c for c in features if c != 'sentiment_bucket']\n",
    "    cat_cols = ['sentiment_bucket']\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\n",
    "            'num',\n",
    "            Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler()),\n",
    "            ]),\n",
    "            num_cols,\n",
    "        ),\n",
    "        (\n",
    "            'cat',\n",
    "            Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ]),\n",
    "            cat_cols,\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=10,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    preds = (proba >= 0.50).astype(int)\n",
    "\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'train_rows': len(X_train),\n",
    "        'test_rows': len(X_test),\n",
    "        'split_date': split_date,\n",
    "        'test_auc': roc_auc_score(y_test, proba),\n",
    "        'test_accuracy': accuracy_score(y_test, preds),\n",
    "        'test_precision': precision_score(y_test, preds, zero_division=0),\n",
    "        'test_recall': recall_score(y_test, preds, zero_division=0),\n",
    "        'test_f1': f1_score(y_test, preds, zero_division=0),\n",
    "        'train_positive_rate': float(y_train.mean()),\n",
    "        'test_positive_rate': float(y_test.mean()),\n",
    "    }])\n",
    "\n",
    "    fitted_preprocessor = model.named_steps['preprocessor']\n",
    "    fitted_classifier = model.named_steps['classifier']\n",
    "\n",
    "    cat_names = (\n",
    "        fitted_preprocessor.named_transformers_['cat']\n",
    "        .named_steps['encoder']\n",
    "        .get_feature_names_out(cat_cols)\n",
    "        .tolist()\n",
    "    )\n",
    "    feature_names = num_cols + cat_names\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': fitted_classifier.feature_importances_,\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, proba)\n",
    "    roc_df = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'threshold': thresholds})\n",
    "\n",
    "    return metrics_df, importance_df, roc_df\n",
    "\n",
    "\n",
    "model_metrics_df, feature_importance_df, roc_df = train_predictive_model(daily, sentiment_clean)\n",
    "\n",
    "model_metrics_df.to_csv(TABLES_DIR / 'predictive_model_metrics.csv', index=False)\n",
    "feature_importance_df.to_csv(TABLES_DIR / 'predictive_feature_importance.csv', index=False)\n",
    "roc_df.to_csv(TABLES_DIR / 'predictive_model_roc_points.csv', index=False)\n",
    "\n",
    "print('Model metrics:')\n",
    "display(model_metrics_df)\n",
    "print('\\nTop features:')\n",
    "display(feature_importance_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf367a09",
   "metadata": {},
   "source": [
    "## Charts and Evidence\n",
    "\n",
    "Create charts used to support the findings and save them for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f681204",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['Fear', 'Greed']\n",
    "\n",
    "# 1) Performance chart\n",
    "plot_data = fg_daily[['sentiment_bucket', 'daily_pnl_usd']].copy()\n",
    "plot_data['winsorized_pnl'] = plot_data.groupby('sentiment_bucket')['daily_pnl_usd'].transform(\n",
    "    lambda s: s.clip(s.quantile(0.05), s.quantile(0.95))\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "sns.boxplot(\n",
    "    data=plot_data,\n",
    "    x='sentiment_bucket',\n",
    "    y='winsorized_pnl',\n",
    "    hue='sentiment_bucket',\n",
    "    order=order,\n",
    "    ax=axes[0],\n",
    "    dodge=False,\n",
    "    palette=['#d66d6d', '#73a3d6'],\n",
    "    legend=False,\n",
    ")\n",
    "axes[0].set_title('Account-Day PnL by Sentiment (Winsorized 5%-95%)')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Daily PnL (USD)')\n",
    "\n",
    "perf_plot = performance_df.set_index('sentiment_bucket').reindex(order)\n",
    "x = np.arange(len(order))\n",
    "width = 0.35\n",
    "axes[1].bar(\n",
    "    x - width / 2,\n",
    "    perf_plot['positive_day_rate'],\n",
    "    width=width,\n",
    "    color='#4c78a8',\n",
    "    label='Positive day rate',\n",
    ")\n",
    "axes[1].set_ylabel('Positive Day Rate')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(order)\n",
    "axes[1].set_title('Hit Rate vs Tail-Loss Proxy')\n",
    "\n",
    "ax2 = axes[1].twinx()\n",
    "ax2.bar(\n",
    "    x + width / 2,\n",
    "    perf_plot['es10_daily_pnl_usd'],\n",
    "    width=width,\n",
    "    color='#e15759',\n",
    "    alpha=0.8,\n",
    "    label='ES10 daily PnL',\n",
    ")\n",
    "ax2.set_ylabel('Expected Shortfall 10% (USD)')\n",
    "\n",
    "h1, l1 = axes[1].get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "axes[1].legend(h1 + h2, l1 + l2, loc='upper right')\n",
    "fig.tight_layout()\n",
    "fig.savefig(CHARTS_DIR / 'performance_fear_vs_greed.png', dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# 2) Behavior chart\n",
    "behavior_plot = behavior_df.set_index('sentiment_bucket').reindex(order).reset_index()\n",
    "metric_specs = [\n",
    "    ('mean_trades_per_account_day', 'Trades / Account-Day'),\n",
    "    ('mean_trade_size_usd', 'Avg Trade Size (USD)'),\n",
    "    ('mean_leverage_proxy', 'Leverage Proxy'),\n",
    "    ('mean_net_long_bias', 'Net Long Bias'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "for axis, (metric, title) in zip(axes.flatten(), metric_specs):\n",
    "    sns.barplot(\n",
    "        data=behavior_plot,\n",
    "        x='sentiment_bucket',\n",
    "        y=metric,\n",
    "        hue='sentiment_bucket',\n",
    "        order=order,\n",
    "        ax=axis,\n",
    "        dodge=False,\n",
    "        palette=['#d66d6d', '#73a3d6'],\n",
    "        legend=False,\n",
    "    )\n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel('Sentiment')\n",
    "    axis.set_ylabel('')\n",
    "fig.tight_layout()\n",
    "fig.savefig(CHARTS_DIR / 'behavior_fear_vs_greed.png', dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# 3) Segment heatmap\n",
    "heatmap_data = segment_summary_df[\n",
    "    segment_summary_df['segment_type'].isin(['leverage_segment', 'activity_segment', 'consistency_segment'])\n",
    "].copy()\n",
    "heatmap_data['row_name'] = (\n",
    "    heatmap_data['segment_type'].str.replace('_segment', '', regex=False)\n",
    "    .str.replace('_', ' ', regex=False)\n",
    "    .str.title()\n",
    "    + ' | '\n",
    "    + heatmap_data['segment']\n",
    ")\n",
    "\n",
    "heatmap_pivot = heatmap_data.pivot(\n",
    "    index='row_name',\n",
    "    columns='sentiment_bucket',\n",
    "    values='mean_daily_pnl_usd',\n",
    ").reindex(columns=order)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "sns.heatmap(\n",
    "    heatmap_pivot,\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='RdYlGn',\n",
    "    center=0,\n",
    "    cbar_kws={'label': 'Mean Daily PnL (USD)'},\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title('Segment Performance by Sentiment')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Segment')\n",
    "fig.tight_layout()\n",
    "fig.savefig(CHARTS_DIR / 'segment_performance_heatmap.png', dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# 4) ROC chart\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(roc_df['fpr'], roc_df['tpr'], color='#4c78a8', linewidth=2, label='Model ROC')\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=1, label='Random baseline')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Next-Day Profitability Model ROC')\n",
    "ax.legend(loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.savefig(CHARTS_DIR / 'predictive_model_roc.png', dpi=180)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3bbf36",
   "metadata": {},
   "source": [
    "## Part C: Actionable Output\n",
    "\n",
    "Summarize findings into strategy rules of thumb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = performance_df.set_index('sentiment_bucket')\n",
    "beh = behavior_df.set_index('sentiment_bucket')\n",
    "seg_focus = segment_summary_df[segment_summary_df['segment_type'].isin(['leverage_segment', 'activity_segment'])].copy()\n",
    "seg_pivot = seg_focus.pivot_table(index=['segment_type', 'segment'], columns='sentiment_bucket', values='mean_daily_pnl_usd')\n",
    "\n",
    "insights = [\n",
    "    f\"Median account-day PnL is higher in Greed ({perf.loc['Greed', 'median_daily_pnl_usd']:,.2f}) than Fear ({perf.loc['Fear', 'median_daily_pnl_usd']:,.2f}).\",\n",
    "    f\"Fear has worse downside tail risk (ES10 {perf.loc['Fear', 'es10_daily_pnl_usd']:,.2f} vs Greed {perf.loc['Greed', 'es10_daily_pnl_usd']:,.2f}).\",\n",
    "    f\"Traders are more active during Fear ({beh.loc['Fear', 'mean_trades_per_account_day']:.1f} vs {beh.loc['Greed', 'mean_trades_per_account_day']:.1f} trades/day) and bias flips long->short.\",\n",
    "    (\n",
    "        \"High leverage-proxy segment underperforms low leverage-proxy in both regimes: \"\n",
    "        f\"Fear {seg_pivot.loc[('leverage_segment', 'High leverage-proxy'), 'Fear']:,.0f} vs \"\n",
    "        f\"{seg_pivot.loc[('leverage_segment', 'Low leverage-proxy'), 'Fear']:,.0f}; \"\n",
    "        f\"Greed {seg_pivot.loc[('leverage_segment', 'High leverage-proxy'), 'Greed']:,.0f} vs \"\n",
    "        f\"{seg_pivot.loc[('leverage_segment', 'Low leverage-proxy'), 'Greed']:,.0f}.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "strategy_rules = [\n",
    "    \"During Fear days, reduce exposure expansion for high leverage-proxy traders and cut position size before increasing frequency.\",\n",
    "    \"Increase trade frequency only for frequent/consistent segments; infrequent traders should avoid sentiment-driven overtrading.\",\n",
    "]\n",
    "\n",
    "print('Insights:')\n",
    "for i, item in enumerate(insights, 1):\n",
    "    print(f\"{i}. {item}\")\n",
    "\n",
    "print('\\nStrategy rules of thumb:')\n",
    "for i, item in enumerate(strategy_rules, 1):\n",
    "    print(f\"{i}. {item}\")\n",
    "\n",
    "# Optional: persist concise write-up\n",
    "summary_path = OUTPUT_DIR / 'summary.md'\n",
    "summary_lines = ['# Hyperliquid Fear/Greed Analysis (Notebook Version)', '', '## Insights']\n",
    "summary_lines += [f'- {x}' for x in insights]\n",
    "summary_lines += ['', '## Strategy Ideas']\n",
    "summary_lines += [f'{i}. {x}' for i, x in enumerate(strategy_rules, 1)]\n",
    "summary_path.write_text('\\n'.join(summary_lines), encoding='utf-8')\n",
    "print(f\"\\nSaved summary to {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec60cd3",
   "metadata": {},
   "source": [
    "## Reproducibility Notes\n",
    "\n",
    "- Run cells top-to-bottom.\n",
    "- Output artifacts are written under `outputs/`.\n",
    "- If matplotlib cache issues appear in restricted environments, run notebook with:\n",
    "  - `MPLBACKEND=Agg`\n",
    "  - `MPLCONFIGDIR=/tmp/mpl`\n",
    "  - `XDG_CACHE_HOME=/tmp`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
